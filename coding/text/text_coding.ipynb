{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data compression fundamentals\n",
    "\n",
    "### Data? ... which data?\n",
    "\n",
    "* We will encode audio, image, and video signals.\n",
    "\n",
    "### Why?\n",
    "\n",
    "* After the digitalization of any signal we get a sequence $s[]$\n",
    "  of samples that represent the signal $s$ with more or less fidelity.\n",
    "  \n",
    "* $s[]$ is encoded using PCM (Pulse Code Modulation), in which\n",
    "  every sample is represented with the same number of bits. For\n",
    "  example, in a CD we have a data-rate of\n",
    "  \n",
    "  $$\n",
    "    (16+16)\\frac{\\text{bits}}{\\text{sample}}\\times\n",
    "    44{.}100\\frac{\\text{samples}}{\\text{second}}=\n",
    "    1{.}411{.}200\\frac{\\text{bits}}{\\text{second}}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources of redundancy in signals\n",
    "\n",
    "* In general, signals has different types of redundancy:\n",
    "\n",
    "    + **Statistical redundancy**: It can be removed using\n",
    "    probabilistic models of the signal producing lossless codecs. The\n",
    "    codecs are also known as *text codecs*.\n",
    "    \n",
    "    + **Spatial/temporal redundancy**: It can be removed using\n",
    "    spatial/temporal models of the signal and produces also lossless\n",
    "    codecs.\n",
    "    \n",
    "    + **Psychological redundancy**: Some information that\n",
    "    signal carry can not be perceived by humans. This kind of\n",
    "    pseudo-redundancy is removed normally by means of quantization,\n",
    "    producing lossy codecs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols, runs, strings, code-words and code-streams\n",
    "\n",
    "* In the context of statistical coding, each sample of $s[]$ is\n",
    "  called a *symbol*.\n",
    "  \n",
    "* Depending on the type of statistical relationship among\n",
    "  symbols, we will speak also about *strings* when we process\n",
    "  more than one symbol and about *runs* when all the symbols are\n",
    "  the same in a string.\n",
    "  \n",
    "* In any case, the output of the encoder is a sequence of\n",
    "  code-words that all together generates a *code-stream*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Run-length encoding\n",
    "\n",
    "* RLE (Run Length Encoding) is a technique that removes the data\n",
    "  redundancy produced by the repetition of symbols. Example:\n",
    "  ```\n",
    "  aaaaa <-> 5a\n",
    "  ```\n",
    "  \n",
    "* There are several versions of RLE codecs, which are different in\n",
    "  the size of the source alphabet or the maximal/minimal length that\n",
    "  the runs can take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 N-ary run length encoding\n",
    "\n",
    "RLE for N-ary alphabets (alphabets of size N), where typically, N=256.\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "1. While there are symbols to encode:\n",
    "    1. Let $s$ the next symbol.\n",
    "    2. Read the next $n$ consecutive symbols equal to $s$.\n",
    "    3. Write the pair $ns$.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "1. While there are $ns$ pairs to decode:\n",
    "    1. Write $n$-times the symbol $s$.\n",
    "    \n",
    "#### Example\n",
    "\n",
    "Runs:\n",
    "```\n",
    "aaaabbbbbaaaaaabbbbbbbcccccc\n",
    "```\n",
    "are encoded as:\n",
    "```\n",
    "4a5b6a7b6c\n",
    "```\n",
    "\n",
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 'a'), (5, 'b'), (6, 'a'), (7, 'b'), (6, 'c')]\n",
      "aaaabbbbbaaaaaabbbbbbbcccccc\n"
     ]
    }
   ],
   "source": [
    "# https://rosettacode.org/wiki/Run-length_encoding#Python\n",
    "    \n",
    "from itertools import groupby\n",
    "def encode(input_string):\n",
    "    return [(len(list(g)), k) for k,g in groupby(input_string)]\n",
    " \n",
    "def decode(lst):\n",
    "    return ''.join(c * n for n,c in lst)\n",
    "\n",
    "x = encode('aaaabbbbbaaaaaabbbbbbbcccccc')\n",
    "print(x)\n",
    "y = decode(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Binary run length encoding\n",
    "\n",
    "* It is not necessary to indicate the next symbol\n",
    "  (only the length) because if a run ends, the other (possible) symbol\n",
    "  start with the next run.\n",
    "  \n",
    "#### Encoder\n",
    "\n",
    "1. Let $s\\leftarrow$ \\texttt{0}.\n",
    "2. While there are bits to encode:\n",
    "    1. Read the next $n$ consecutive bits equal to $s$.\n",
    "    2. Write $n$.\n",
    "    3. $s\\leftarrow (s+1)~\\text{modulus}~2$.\n",
    "    \n",
    "#### Decoder\n",
    "\n",
    "1. Let $s\\leftarrow$ \\texttt{0}.\n",
    "2. While there are items $n$ to decode:\n",
    "    1. Write $n$ bits equal to $s$.\n",
    "    2. $s\\leftarrow (s+1)~\\text{modulus}~2$.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Runs:\n",
    "```\n",
    "0000111110000001111111000000\n",
    "```\n",
    "are encoded as::\n",
    "```\n",
    "4 5 6 7 6\n",
    "```\n",
    "\n",
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 [MPN-5 run length encoding](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=held+data+compression+techniques+applications&btnG=)\n",
    "\n",
    "* Created by Microcom Inc. for the MNP (Microcom Networking\n",
    "  Protocol) 5.\n",
    "\n",
    "#### Codec\n",
    "\n",
    "* The behavior of the codec can be easily defined with the\n",
    "  following examples:\n",
    "  \n",
    "```\n",
    "Input     Output\n",
    "--------- ---------\n",
    "ab        ab\n",
    "aab       aab\n",
    "aaab      aaa0b\n",
    "aaaab     aaa1b\n",
    "aaaaab    aaa2b\n",
    ":         :\n",
    "a^nb      aaa(n-3)b\n",
    "```\n",
    "\n",
    "#### Example\n",
    "\n",
    "Runs:\n",
    "```\n",
    "aaaabbbbbaaaaaabbbbbbbcccccc\n",
    "```\n",
    "are encoded as:\n",
    "```\n",
    "aaa1bbb2aaa3bbb4\n",
    "```\n",
    "\n",
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 [Burrows-Wheeler transform](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Burrows+M%2C+Wheeler+DJ%3A+A+Block+Sorting+Lossless+Data+Compression+Algorithm.&btnG=)\n",
    "\n",
    "* BWT (Burrows-Wheeler Transform) is an algorithm that inputs\n",
    "  a string and outputs:\n",
    "  1. A different string with the same symbols (with longer runs),\n",
    "    but with a different order.\n",
    "  2. An index.\n",
    "  \n",
    "  \n",
    "* There is an inverse transform that, using the output of the\n",
    "  forward transform, recover the original string.\n",
    "  \n",
    "* The transformed string tends to have longer runs.\n",
    "\n",
    "* The length of the runs in proportional to the correlation\n",
    "  between the symbols and the length of the input.\n",
    "  \n",
    "### Forward transform\n",
    "\n",
    "Let $B$ the block-size in symbols:\n",
    "\n",
    "1. Read $B$ symbols.\n",
    "2. Build a square matrix of size $B\\times B$ where the first row is\n",
    "  the original sequence, the second one is the same sequence but\n",
    "  cyclically shifted one symbol to the left, and so on ...\n",
    "3. Sort lexicographically the matrix by rows.\n",
    "4. Search in the last column the row in which the first symbol of\n",
    "  the original sequence it is found. This is the index $i$.\n",
    "5. Output $i$ and the last column $O[]$.\n",
    "\n",
    "#### Encoding example\n",
    "<img src=\"text_coding/BWT_example.svg\" style=\"width: 800px;\"/>\n",
    "\n",
    "### Inverse transform\n",
    "\n",
    "1. Sort $O[]$ over $S[]$.\n",
    "2. Compute $T[]$ where if $S[j]=O[l]$ (being $l$ the first symbol\n",
    "  of $O[]$ that matches this condition), then $T[j]=l$. Notice that\n",
    "  all of symbols of $T[]$ have to be different.\n",
    "3. Let $k\\leftarrow i$.\n",
    "4. Execute $B$ times:\n",
    "    1. Output $O[k]$.\n",
    "    2. $k\\leftarrow T[k]$.\n",
    "    \n",
    "#### Decoding example\n",
    "<img src=\"text_coding/BWT_example_decod.svg\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 baaaaaabbabaacaab\n",
      "ababcbababaaaaaaa\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/dmckean/9723bc06254809e9068f\n",
    "\n",
    "def bwt_encode(s):\n",
    "    n = len(s)\n",
    "    m = sorted([s[i:n]+s[0:i] for i in range(n)])\n",
    "    I = m.index(s)\n",
    "    L = ''.join([q[-1] for q in m])\n",
    "    return (I, L)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def bwt_decode(I, L):\n",
    "    n = len(L)\n",
    "    X = sorted([(i, x) for i, x in enumerate(L)], key=itemgetter(1))\n",
    "\n",
    "    T = [None for i in range(n)]\n",
    "    for i, y in enumerate(X):\n",
    "        j, _ = y\n",
    "        T[j] = i\n",
    "\n",
    "    Tx = [I]\n",
    "    for i in range(1, n):\n",
    "        Tx.append(T[Tx[i-1]])\n",
    "\n",
    "    S = [L[i] for i in Tx]\n",
    "    S.reverse()\n",
    "    return ''.join(S)\n",
    "\n",
    "index, encoded = bwt_encode('ababcbababaaaaaaa')\n",
    "print (index, encoded)\n",
    "decoded = bwt_decode(index, encoded)\n",
    "print (decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. String encoding\n",
    "\n",
    "### How it works?\n",
    "\n",
    "* We replace strings by code-words of less length.\n",
    "* Strings are searched in a dictionary and the sequence of positions of the strings in the dictionary form the code-strem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 LZ77 [[J. Ziv and A. Lempel, 1977]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Ziv+Lempel+universal+sequential+data+compression+1977&btnG=)\n",
    "\n",
    "* In 1977, Jacov Ziv and Abraham Lempel propose the LZ77 algorithm.\n",
    "* In the eighties, a branch of LZ77 known as LZSS and is\n",
    "  implemented by Haruyasu Yoshizaki in the program LHARC, discovering\n",
    "  the possibilities of the LZ77 encoding.\n",
    "* After that, a large number of text compressors have been based\n",
    "  on the LZ77 idea (or a variation of it). Some of the most famous\n",
    "  are: `ARJ`, `RAR`, `gzip` and `7z`.\n",
    "* LZ77 processes a sequence of symbols using the structure:\n",
    "\n",
    "<img src=\"text_coding/LZ77.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "* The dictionary and the look-ahead buffer have a fixed size and\n",
    "  can be considered as a sliding window, where the input of a new\n",
    "  symbol generates the output of the oldest one, which becomes the\n",
    "  newest symbol of the dictionary.\n",
    "  \n",
    "#### Encoder\n",
    "\n",
    "1. Let $I$ the length of the dictionary and $J$ the length of the\n",
    "  buffer.\n",
    "2. Input the first $J$ symbols in the buffer.\n",
    "3. While the input is not exhausted:\n",
    "    1. Let $i$ the position in the dictionary of the first $j$\n",
    "    symbols of the buffer and $k$ the symbol that makes that $j$ can\n",
    "    not be larger.\n",
    "    2. Output $ijk$.\n",
    "    3. Input the next $j+1$ in the buffer.\n",
    "    \n",
    "#### Decoder\n",
    "\n",
    "1. While the code-words $ijk$ are not exhausted:\n",
    "    1. Output the $j$ symbols extracted from the position $i$ in the\n",
    "    dictionary.\n",
    "    2. Output $k$.\n",
    "    3. Introduce all the decoded symbols into the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "<img src=\"text_coding/LZ77_encoding_example.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"text_coding/LZ77_decoding_example.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "* Parameters $I$ and $J$ control the performance\n",
    "  of the algorithm. They should be large enough to guarantee the\n",
    "  matching of long strings, but should keep small in order to reduce\n",
    "  the number of bits of the code-words $ijk$. Typical sizes are:\n",
    "  $\\log_2(I)=12.0$ and $\\log_2(J)=4.0$.\n",
    "\n",
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To-do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 LZ78 [[J. Ziv and A. Lempel, 1978]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Ziv+Lempel+1978&btnG=)\n",
    "\n",
    "* In 1978, Ziv and Lempel published the LZ78 algorithm.\n",
    "\n",
    "* LZ89 represents the dictionary in a recursive way with the idea\n",
    "  of improving the search of the strings in the dictionary. Now, each\n",
    "  entry in the dictionary is a pair $wk$, where $w$ is a pointer to\n",
    "  the dictionary and $k$ is a symbol. In fact, each entry $wk$\n",
    "  represents the string that results from the concatenation of string\n",
    "  $w$ and $k$, where $w$ can be recursively computed as we have found\n",
    "  $wk$.\n",
    "  \n",
    "* We will denote \\textit{string}$(w)$ to the string that $w$\n",
    "  represents.\n",
    "  \n",
    "* The empty string is obtained by \\textit{string}$(0)$.\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "1. $w\\leftarrow 0$.\n",
    "2. While the input is not exhausted:\n",
    "    1. $k\\leftarrow$ next input symbol.\n",
    "    2. If $wk$ is found in the dictionary, then:\n",
    "        1. $w\\leftarrow$ address of $wk$ in the dictionary.\n",
    "    3. Else:\n",
    "        1. Output $wk$.\n",
    "        2. Insert $wk$ in the dictionary.\n",
    "        3. $w\\leftarrow 0$.\n",
    "        \n",
    "#### Decoder\n",
    "\n",
    "1. While the input is not exhausted:\n",
    "    1. Input $wk$.\n",
    "    2. Output $\\text{string}(w)$.\n",
    "    3. Output $k$.\n",
    "    4. Insert $wk$ in the dictionary.\n",
    "    \n",
    "#### Example\n",
    "\n",
    "<img src=\"text_coding/LZ78_encoding_example.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"text_coding/LZ78_decoding_example.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 LZW [[T.A. Welch, 1984]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Terry+Welch+1984&btnG=)\n",
    "\n",
    "* In 1984 Terry A. Welch proposes the LZW algorithm,\n",
    "  which is an improved version of the LZ89 algorithm that does not\n",
    "  writes raw symbols ($k$) to the code-stream.\n",
    "\n",
    "* LZW was selected as encoding engine for the GIF (Graphics\n",
    "  Interchange Format), and for the compressor `compress`.\n",
    "  \n",
    "* The dictionary is initially filled with the $2^k$ possible\n",
    "  symbols (*roots*), that are stored in entries $0\\cdots255$.\n",
    "  \n",
    "#### Encoder\n",
    "\n",
    "1. $w\\leftarrow$ next input symbol.\n",
    "2. While the input is not exhausted:\n",
    "    1. $k\\leftarrow$ next input symbol.\n",
    "    2. If $wk$ is found in the dictionary, then:\n",
    "        1. $w\\leftarrow$ address of $wk$ in the dictionary.\n",
    "    3. Else:\n",
    "        1. Output $\\leftarrow w$.\n",
    "        2. Insert $wk$ in the dictionary.\n",
    "        3. $w\\leftarrow k$.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "1. $code\\leftarrow$ first input code-word.\n",
    "2. Output $code$.\n",
    "3. $old\\_code\\leftarrow code$.\n",
    "4. While the input is not exhausted:\n",
    "    1. $code\\leftarrow$ next input code-word.\n",
    "    2. $w\\leftarrow old\\_code$.\n",
    "    3. If $code$ is found in the dictionary, then:\n",
    "        1. Output string$(code)$.\n",
    "    4. Else:\n",
    "        1. Output string$(w)$.\n",
    "        2. Output $k$.\n",
    "    5. $k\\leftarrow$ first symbol of the last output.\n",
    "    6. Insert $wk$ in the dictionary.\n",
    "    7. $old\\_code\\leftarrow code$.\n",
    "    \n",
    "#### Example\n",
    "\n",
    "<img src=\"text_coding/LZW_encoding_example.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"text_coding/LZW_decoding_example.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "#### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://rosettacode.org/wiki/LZW_compression#Python\n",
    "\n",
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Symbol encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works?\n",
    "\n",
    "* We can compress if each symbol is translated by code-words and,\n",
    "  in average, the lengths of the code-words are smaller than the\n",
    "  length of the symbols.\n",
    "  \n",
    "* The encoder and the decoder have a probabilistic model $M$ which\n",
    "  says to the variable-length encoder ($C$)/decoder($C^{-1}$) the\n",
    "  probability $p(s)$ of each symbol $s$.\n",
    "  \n",
    "<img src=\"text_coding/compresion_entropica.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "* The most probable symbols are represented by the shorter\n",
    "  code-words and viceversa.\n",
    "  \n",
    "### Bits\n",
    "\n",
    "* Data is the representation of the information.\n",
    "\n",
    "* Lossless data compression uses a shorter representation of the\n",
    "  information.\n",
    "  \n",
    "* By definition, a bit of data stores a bit of information if and\n",
    "  only if it represents the occurrence of an equiprobable event (an\n",
    "  event that can be true or false with the same probability).\n",
    "  \n",
    "* By definition, a symbol $s$ with probability $p(s)$ stores\n",
    "\n",
    "\\begin{equation}\n",
    "  I(s)=-\\log_2 p(s) \\tag{Eq:symbol_information}\n",
    "  \\label{eq:symbol_information}\n",
    "\\end{equation}\n",
    "  \n",
    "  bits of information.\n",
    "\n",
    "* The length of the code-word depends on the probability as:\n",
    "\n",
    "<img src=\"text_coding/prob_vs_long.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Entropy\n",
    "\n",
    "* The entropy $H(S)$ measures the amount of information per\n",
    "  symbol that a source of information $S$ produces, in average, i.e.\n",
    "  \n",
    "\\begin{equation}\n",
    "  H(S) = \\frac{1}{N}\\sum_{s=1}^{N} p(s)\\times I(s)\n",
    "\\end{equation}\n",
    "\n",
    "  bits-of-information/symbol, where $N$ is the size of the source\n",
    "  alphabet (number of different symbols).\n",
    "\n",
    "### C.1 Universal coding\n",
    "\n",
    "* An ideal entropy encoder should represent each symbol $s$ with a\n",
    "  number of bits that $I(s)$ says (see \\ref{eq:symbol_information}).\n",
    "  \n",
    "* This system will be 100\\% efficient is the guesses are\n",
    "  equiprobable!\n",
    "  \n",
    "#### Encoding of a symbol\n",
    "\n",
    "1. While the decoder does not know the symbol:\n",
    "    1. Assert something about the symbol that allows to the decoder\n",
    "    to minimize the uncertainty of finding that symbol. This guess\n",
    "    should have the same probability of to be true or false.\n",
    "    2. Output a bit of code that says if the last guess is true or\n",
    "    false.\n",
    "    \n",
    "#### Decoding of a symbol\n",
    "\n",
    "1. While the symbol is not known without uncertainty:\n",
    "    1. Make the same guess that the encoder.\n",
    "    2. Input a bit of code that represents the result of the last\n",
    "    guess.\n",
    "\n",
    "#### Example\n",
    "\n",
    "* Let's suppose that we use the Spanish alphabet. Humans know that\n",
    "  symbols does not form words in any order (this fact can help us to\n",
    "  formulate the following VLC (Variable Length Codec)):\n",
    "  \n",
    "* In Spanish there are 28 letters. Therefore, to encode, for example,\n",
    "  the word \"preciosa\", the first symbol ``p'' can be represented by\n",
    "  it index inside the Spahish alphabet with a code-word of 5 bits. In\n",
    "  this try, the encoding is not a very efficient, but this one is the\n",
    "  first letter ... For the second one ``r'' we can see (using a\n",
    "  Spanish dictionary) that after a ``p'', the following symbols are\n",
    "  possible: (1) ``a'', (2) ``e'', (3) ``i'', (4) ``l'', (5) ``n'', (5)\n",
    "  ``o'', (7) ``r'', (8) ``s'' and (9) ``u''. Therefore, we don't need\n",
    "  5 bits now, 4 are enough.\n",
    "  \n",
    "<img src=\"text_coding/universal_coding_example.svg\" style=\"width: 300px;\"/>\n",
    "\n",
    "* The compression ratio has been 40/25:1 (``preciosa'' has 8\n",
    "  letters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### C.2 Shannon-Fano coding\n",
    "\n",
    "* At the end of the 40's, Claude E. Shannon (Bell Labs) and\n",
    "  R.M. Fano (MIT) developed an VLC codec.\n",
    "  \n",
    "#### Encoder\n",
    "\n",
    "1. Sort the symbols using their probabilities.\n",
    "2. Split the set of symbols into two subsets in a way in which the\n",
    "   each subset have the same total probability.\n",
    "3. Assign a different bit to each set.\n",
    "4. Repeat the previous procedure to each subset until the size of\n",
    "   each subset is equal to 1.\n",
    "   \n",
    "#### Decoder\n",
    "\n",
    "To-do.\n",
    "\n",
    "#### Example\n",
    "\n",
    "* Let's use the next probabilistic model:\n",
    "<img src=\"text_coding/shannon-fano_example.svg\" style=\"width: 120px;\"/>\n",
    "Using it, this is the coding:\n",
    "<img src=\"text_coding/shannon-fano_example-coding.svg\" style=\"width: 850px;\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3 Huffman coding [[Huffman, 1952](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=huffman+method+codes+1952&btnG=)\n",
    " \n",
    "* Optimal performance (better than Shannon-Fano) when a integer\n",
    "  number of bits is assigned to each symbol.\n",
    "* The VLC codec builds a binary tree where the symbols are stored\n",
    "  in the leafs and the distance of each symbol to the root of the tree\n",
    "  is $\\lceil\\log_2(p(s))\\rceil$.\n",
    "* After label each binary branch in the tree, the Huffman\n",
    "  code-word for the symbol $s$ is the sequence of bits (labels) that\n",
    "  we must use to travel from the root to the $s$-leaf.\n",
    " \n",
    "#### Building Huffman trees\n",
    "\n",
    "1. Create a list of nodes. Each node stores a symbol and its\n",
    "   probability.\n",
    "2. While the number of nodes in the list > 1:\n",
    "    1. Extract from the list the 2 nodes with the lowest probability.\n",
    "    2. Insert in the list a new node (that is the root of a binary\n",
    "       tree) whose probability is the sum of the probability of its\n",
    "       leafs.\n",
    "       \n",
    "#### Example\n",
    "\n",
    "<img src=\"text_coding/huffman_ejemplo.svg\" style=\"width: 800px;\"/>\n",
    "\n",
    "#### Limits\n",
    "\n",
    "* Any Huffman code satisfies that\n",
    "  \\begin{equation}\n",
    "    l\\big(c(s)\\big) = \\lceil I(s)\\rceil, \\tag{Eq:Huffman}\n",
    "    \\label{eq:Huffman}\n",
    "  \\end{equation}\n",
    "  where $l\\big(c(s)\\big)$ is the length of the code-word assigned to\n",
    "  the symbol $s$.\n",
    "  \n",
    "* This implies that, with every encoded symbol, up to 1 bit of\n",
    "  redundant data can be introduced.\n",
    "  \n",
    "* This is a problem that grows when the size of the alphabet is\n",
    "  small. In the extreme case, for binary source alphabets, the Huffman\n",
    "  coding does not change the length of the original representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.4 Arithmetic coding\n",
    "\n",
    "* Arithmetic coding relaxes the Equation \\ref{eq:Huffman},\n",
    "  verifying that, for every encoded symbol, \n",
    "  \\begin{equation}\n",
    "    l\\big(c(s)\\big) = I(s), \\tag{Eq:arithmetic}\n",
    "    \\label{eq:arithmetic}\n",
    "  \\end{equation}\n",
    "  i.e. the number of bits of data (code-word) assigned by the encoder\n",
    "  is equal to the number of bits of information that the symbol\n",
    "  represent.\n",
    "\n",
    "<img src=\"text_coding/comparacion.svg\" style=\"width: 800px;\"/>\n",
    "\n",
    "* It can be said that, the arithmetic coding is optimal because\n",
    "  the average length of an arithmetic code is equal to the entropy of\n",
    "  the information source, measured in bits/symbol.\n",
    "\n",
    "#### Ideal encoder\n",
    "\n",
    "1. Let $[L,H)\\leftarrow [0.0,1.0)$ an interval of real numbers.\n",
    "2. While the input is not exhausted:\n",
    "    1. Split $[L,H)$ into so many sub-intervals as different symbols\n",
    "       are in the alphabet. The size of each sub-interval is proportional\n",
    "       to the probability of the corresponding symbol.\n",
    "    2. Select the sub-interval $[L',H')$ associated with the encoded\n",
    "       symbol.\n",
    "    3. $[L,H)\\leftarrow [L',H')$.\n",
    "3. Output a real number $x\\in[L,H)$ (the arithmetic\n",
    "   code-stream). The number of decimals of $x$ should be large enough\n",
    "   to distinguish the final sub-interval $[L,H)$ from the rest of\n",
    "   possibilities.\n",
    "   \n",
    "#### Example\n",
    "\n",
    "* Imagine a binary sequence, where $p(\\text{A})=3/4$ and\n",
    "  $p(\\text{B})=1/4$. Compute the arithmetic code of the sequences A, B,\n",
    "  AA, AB, BA y BB.\n",
    "  \n",
    "<img src=\"text_coding/aritmetica_ejemplo.svg\" style=\"width: 500px;\"/>\n",
    "\n",
    "#### Incremental transmission\n",
    "\n",
    "* It is not necessary to wait for the end of the encoding to\n",
    "  generate the arithmetic code. When we work with binary\n",
    "  representations of the real numbers $L$ and $H$, their most\n",
    "  significant bits are identical as the interval is reduced. These\n",
    "  bits are going to contribute to the arithmetic code, therefore, they\n",
    "  can be output as soon as they are known.\n",
    "  \n",
    "  For example, when the symbol B is encoded, a code-bit 1 can be\n",
    "    output because any sequence of symbols that start with B have a\n",
    "    code-word that begins by 1.\n",
    "    \n",
    "* When the most significant bits of $L$ and $H$ are output, the\n",
    "  bits of each register are shifted to the left, and new bits need to\n",
    "  be inserted. The results is an automatic zoom of the selected\n",
    "  sub-interval.\n",
    "\n",
    "  Following with the previous example, the register shifting generates\n",
    "    an ampliation of the $[0.50,1.00)$ interval to the $[0.00,1.00)$.\n",
    "    \n",
    "#### The ideal decoder\n",
    "\n",
    "1. Let $[L,H)\\leftarrow [0.0,1.0)$ the initial interval.\n",
    "2. While the input is not exhausted:\n",
    "    1. Split $[L,H)$ into so many sub-intervals as different symbols\n",
    "       are in the alphabet. The size of each sub-interval is proportional\n",
    "       to the probability of the corresponding symbol.\n",
    "    2. Input so many bits of $x$ as they are needed to:\n",
    "        1. Select the sub-interval $[L',H')$ that contains $x$.\n",
    "        2. Output the symbol that $[L',H')$ represents.\n",
    "        3. $[L,H)\\leftarrow[L',H')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.5 Probabilistic models\n",
    "\n",
    "* In order to use any of the previous VLCs, a probabilistic model is needed.\n",
    "\n",
    "### Static models\n",
    "\n",
    "* The simplest models because the probabilities of the symbols\n",
    "  remain constant.\n",
    "* The variable-length codec can be precomputed.\n",
    "* If the last premise is true, the entropy codec is efficient an\n",
    "  fast. For this reason, static models are very common in codecs such\n",
    "  as JPEG, MPEG (audio and video), etc.\n",
    "  \n",
    "### Adaptive models\n",
    "\n",
    "* The probabilities of the symbols are computed in run-time.\n",
    "* In general, the compression ratios that an adaptive model \n",
    "  get are better than the static model's ones because the\n",
    "  probabilities of the symbols are localy computed.\n",
    "  \n",
    "#### Encoding\n",
    "\n",
    "1. Asign the same probability to all the symbols.\n",
    "2. While the input if not exhausted:\n",
    "    1. Encode the next symbol.\n",
    "    2. Update (increase) its probability.\n",
    "\n",
    "#### Decoding\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. Decode the next symbol.\n",
    "    2. Identical to the step 2.b of the encoder.\n",
    "    \n",
    "### Initially empty models\n",
    "\n",
    "* The smaller the number of symbols used by the model, the higher\n",
    "  the probabilities, and therefore, the better the compression ratios.\n",
    "* An initially empty model only stores the ESC(cape) symbol, a\n",
    "  symbol that it is used by the encoder only when a new symbol is\n",
    "  found.\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "1. Set the probability of the ESC to $1.0$ (and the probability of\n",
    "   the rest of the symbols to $0.0$).\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next symbol.\n",
    "    2. If $s$ has been found before, then:\n",
    "        1. Encode $s$ and output $c(s)$.\n",
    "    3. Else:\n",
    "        1. Output $c(\\mathrm{ESC})$.\n",
    "        2. Output a raw symbol $s$.\n",
    "    4. Update $p(s)$.\n",
    "    \n",
    "#### Decoder\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c(s)\\leftarrow $ next code-word.\n",
    "    2. Decode $s$.\n",
    "    3. If $s=$ ESC, then:\n",
    "        1. Input a raw symbol $s$.\n",
    "    4. Update $p(s)$.\n",
    "    5. Output $s$.\n",
    "    \n",
    "### Models with memory\n",
    "\n",
    "* In most cases, the probability of a symbol depends on its\n",
    "  neighborhood (context).\n",
    "* The higher the memory of the model (the context), the higher the\n",
    "  accuracy of the predictions (probabilities), and therefore, the\n",
    "  lower the length of the code-words \\cite{Cleary.PPM}.\n",
    "* Let ${\\cal C}[i]$ the last $i$ encoded symbols and\n",
    "  $p(s|{\\cal C}[i])$ the probability that the symbol $s$ follows\n",
    "  the context ${\\cal C}[i]$.\n",
    "* Let $k$ the maximal order of the prediction (i.e. the largest\n",
    "  number of symbols of ${\\cal C}[]$ that are going to be used as the\n",
    "  actual context). Notice that ${\\cal C}[0]=\\varnothing$ and the model\n",
    "  has no memory.\n",
    "* We suppose that arithmetic coding is used and therefore, when we\n",
    "  input or output $c(s)$, we are transmitting $I(s)$ bits of code.\n",
    "* Let $r$ the size of the source alphabet.\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "1. Create an empty model for every context $0\\le i \\le k$.\n",
    "2. Create an non-empty model for $k=-1$.\n",
    "3. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ Input$_{\\log_2(r)}$.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where\n",
    "       $i\\leftarrow 0$).\n",
    "    3. While $p(s|{\\cal C}[i])=0$ (it is the first time that $s$ follows\n",
    "       ${\\cal C}[i]$):\n",
    "        1. Output $\\leftarrow c(\\text{ESC}|{\\cal C}[i])$.\n",
    "        2. Update $p(\\text{ESC}|{\\cal C}[i])$.\n",
    "        3. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context).\n",
    "        4. $i\\leftarrow i-1$.\n",
    "    4. Output $\\leftarrow c(s|{\\cal C}[i])$. The symbols that were in\n",
    "       contexts with order $>i$ must be excluded of the actual (${\\cal C}[i]$) context because $s$ is not none of them.\n",
    "    5. If $i\\ge 0$, update $p(s|{\\cal C}[i])$.\n",
    "    \n",
    "#### Example\n",
    "\n",
    "* Let $r=256$ the size of the source alphabet.\n",
    "\n",
    "* The probabilistic model $M[{\\cal C}[-1]]$ (for the special context\n",
    "  ${\\cal C}[-1]$) is non adaptative, non empty and has an special symbol EOF\n",
    "  (End Of File) that is going to be used when the compression has\n",
    "  finished:\n",
    "  $$M[{\\cal C}[-1]]=\\{0,1~1,1~\\cdots~\\mathtt{a},1~\\mathtt{b},1~\\cdots~255,1~\\text{EOF},1\\}.$$\n",
    "  In a pair $a,b$, $a$ is the symbol and $b$ is its probability (counts).\n",
    "\n",
    "* $M[{\\cal C}[0]]$ is adaptative and empty:\n",
    "  $$M[{\\cal C}[0]]=\\{\\text{ESC},1\\}.$$\n",
    "\n",
    "* In this example (for the sake of the simplicity), the maximal\n",
    "  order of the prediction $k=1$ (we only remember the previous\n",
    "  symbol). Therefore, there are $r=256$ probabilistic models:\n",
    "  $$M[{\\cal C}[1]]=\\{\\text{ESC},1\\}, 0\\le {\\cal C}[1]\\le r.$$\n",
    "  \n",
    "* Encoding of the first symbol (\\texttt{a}) (see Figure~\\ref{fig:PPM}):\n",
    "\n",
    "1. [3.A] $s\\leftarrow$ \\texttt{a}.\n",
    "2. [3.B] $i\\leftarrow 0$ (we don't know the previous symbol).\n",
    "3. [3.C] $p(\\mathtt{a}|{\\cal C}[0])=0$ (the context only has the ESC).\n",
    "4. [3.C.a] Output $\\leftarrow c(\\text{ESC}|{\\cal C}[0])$ (althought\n",
    "    $l(c(\\text{ESC}|{\\cal C}[0]))=0$).\n",
    "5. [3.C.b] Update $p(\\text{ESC}|{\\cal C}[0])$ (now, the count of ESC is\n",
    "    2).\n",
    "6. [3.C.c] Insert \\texttt{a} into\n",
    "    $M[{\\cal C}[0]]=\\{\\mathsf{ESC},2~\\mathtt{a},1\\}$.\n",
    "7. [3.C.d] $i\\leftarrow -1$.\n",
    "8. [3.c] $p(\\mathtt{a}|{\\cal C}[-1])\\neq 0$.\n",
    "9. [3.d] Output $\\leftarrow c(\\texttt{a}|{\\cal C}[-1])$ where\n",
    "    $p(\\texttt{a}|{\\cal C}[-1]) = 1/(256+1)$.\n",
    "    \n",
    "* Encoding of the second symbol (\\texttt{b}):\n",
    "\n",
    "1. [3.a] $s\\leftarrow$ \\texttt{b}.\n",
    "2. [3.b] $i\\leftarrow 1$.\n",
    "3. [3.c] $p(\\mathtt{b}|{\\cal C}[1])=0$ because ${\\cal C}[1]=\\texttt{a}$ and\n",
    "   $M[\\texttt{a}]=\\{\\text{ESC},1\\}$.\n",
    "4. [3.c.i] Output $\\leftarrow c(\\text{ESC}|\\texttt{a})$ (althought\n",
    "   $l(c(\\text{ESC}|\\texttt{a}))=0$).\n",
    "5. [3.c.ii] Update $p(\\text{ESC}|\\texttt{a})$ (now, the count of ESC is 2).\n",
    "6. [3.c.iii] Insert \\texttt{b} into $M[\\texttt{a}]=\\{\\text{ESC},2~ \\texttt{b},1\\}$.\n",
    "7. [3.c.iv] $i\\leftarrow 0$.\n",
    "8. [3.c] $p(\\mathtt{b}|{\\cal C}[0])=0$ because\n",
    "   $M[{\\cal C}[0]]=\\{\\mathsf{ESC},2~\\texttt{a},1\\}$.\n",
    "9. [3.c.i] Output $\\leftarrow c(\\text{ESC}|{\\cal C}[0])$ where\n",
    "   $p(\\text{ESC}|{\\cal C}[0]) = 2/3$.\n",
    "10. [3.c.ii] Update $p(\\text{ESC}|{\\cal C}[0])$ (now, the count of ESC is\n",
    "    3).\n",
    "11. [3.c.iii] Insert \\texttt{b} into $M[{\\cal C}[0]] = \\{\\text{ESC},3~\n",
    "    \\texttt{a},1~ \\texttt{b},1\\}$.\n",
    "12. [3.c.iv] $i\\leftarrow -1$.\n",
    "13. [3.c] $p(\\mathtt{b}|{\\cal C}[-1])\\neq 0$.\n",
    "14. [3.d] Output $\\leftarrow c(\\texttt{b}|{\\cal C}[-1])$ where\n",
    "    $p(\\mathtt{b}|{\\cal C}[-1]) = 1/r$. The symbol \\texttt{a} has been\n",
    "    excluded in the calculus of the probability of \\texttt{b} because\n",
    "    $\\texttt{a}\\in M[{\\cal C}[0]] = \\{\\text{ESC},3~ \\texttt{a},1~\n",
    "    \\texttt{b},1\\}$.\n",
    "\n",
    "<img src=\"text_coding/PPM_example.svg\" style=\"width: 800px;\"/>\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "1. Equal to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    2. $s\\leftarrow$ next decoded symbol.\n",
    "    3. While $s=\\text{ESC}$:\n",
    "        1. Update $p(\\text{ESC}|{\\cal C}[i])$.\n",
    "        2. $i\\leftarrow i-1$.\n",
    "        3. $s\\leftarrow$ next decoded symbol.\n",
    "    4. Update $p(s|{\\cal C}[i])$.\n",
    "    5. While $i<k$:\n",
    "        1. $i\\leftarrow i+1$.\n",
    "        2. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.6 MTF (Move To Front) transform\n",
    "\n",
    "* Inputs a sequence of symbols and outputs a sequence of symbols.\n",
    "\n",
    "* The size (in bits of data) for each sequence is the same.\n",
    "\n",
    "* The entropy of the output is lower that the input's one.\n",
    "\n",
    "* Performs a change in the representation of the symbols where\n",
    "  those symbols that have a high probability of occurrency are\n",
    "  ``moved'' in the source alphabet towards decreasing positions.\n",
    "\n",
    "* The probability density function follows an exponential\n",
    "  distribution with a slope $\\lambda$ where\n",
    "\\begin{equation}\n",
    "  f(x.\\lambda) = \\left\\{ \\begin{array}{ll}\n",
    "      \\lambda e^{-\\lambda x} & \\mbox{if $x \\geq 0$};\\\\\n",
    "      0 & \\mbox{if $x < 0$}.\\end{array} \\right.\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"text_coding/exponential.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Forward transform\n",
    "\n",
    "1. Create a list $L$ with the symbols of the source alphabet\n",
    "  where $$L[s]\\leftarrow s; 0\\le s\\le r.$$\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next input symbol.\n",
    "    2. $c\\leftarrow$ position of $s$ in $L$ ($L[c]=s$).\n",
    "    3. Output $\\leftarrow c$.\n",
    "    4. Move $s$ to the front of $L$.\n",
    "    \n",
    "### Inverse transform\n",
    "\n",
    "1. The step 1 of the forward transform.\n",
    "2. While the input is not exausted:\n",
    "    1. $c\\leftarrow$ next input code.\n",
    "    2. $s\\leftarrow L[c]$.\n",
    "    3.  Output $s$.\n",
    "    4. The step 2.C of the forward transform.\n",
    "    \n",
    "#### Example\n",
    "\n",
    "<img src=\"text_coding/MTF_example.svg\" style=\"width: 250px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.7 Context-based Text Predictive transform\n",
    "\n",
    "* The MTF uses a model where a symbol that has happened only one\n",
    "  time can get a index-code that is lower than the index-code of a\n",
    "  symbol that has been found thousands of times :-(\n",
    "\n",
    "* We can solve this problem if the positions of the symbols are\n",
    "  determined by their probability. In other words, the list $L$ will\n",
    "  be sorted by the ocurrence of the symbols.\n",
    "  \n",
    "### 0-order encoder\n",
    "\n",
    "1. The step 1 of the MTF transform, although now every node of the\n",
    "   list stores also a count of the symbol.\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next input symbol.\n",
    "    2. $c\\leftarrow$ position of $s$ in $L$ (the prediction error).\n",
    "    3. Output $\\leftarrow c$.\n",
    "    4. Update the count of $L[c]$ (the count of $s$) and keep sorted $L$.\n",
    "\n",
    "#### Example\n",
    "\n",
    "<img src=\"text_coding/TPT_example.svg\" style=\"width: 450px;\"/>\n",
    "\n",
    "### 0-order decoder\n",
    "\n",
    "1. The step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c\\leftarrow$ next input code.\n",
    "    2. $s\\leftarrow L[c]$.\n",
    "    3. Output $s$.\n",
    "    4. Step 2.D of the encoder.\n",
    "    \n",
    "### $N$-order encoder\n",
    "\n",
    "1. Let ${\\cal C}[i]$ the context of $s$ and $L_{{\\cal C}[i]}$ the\n",
    "   list for that context. If $i>0$ then the lists are empty, else, the\n",
    "   list is full and the count of every node is $0$.\n",
    "2. Let $N$ the order of the prediction.\n",
    "3. Let $H=\\varnothing$ a list of tested symbols. All symbols in $H$\n",
    "   must be different.\n",
    "4. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ the next input symbol.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    3. While $s\\notin L_{{\\cal C}[i]}$:\n",
    "        1. $H\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})$. (reduce$()$ deletes the repeated nodes).\n",
    "        2. Update the count of $s$ in $L_{{\\cal C}[i]}$ and keep sorted it.\n",
    "        3. $i\\leftarrow i-1$.\n",
    "    4. Let $c$ the position of $s$ en $L_{{\\cal C}[i]}$.\n",
    "    5. $c\\leftarrow c+$ symbols of $H-L_{{\\cal C}[i]}$. In this\n",
    "       way, the decoder will know the length of the context where $s$\n",
    "       happens and does not count the same symbol twice.\n",
    "    6. Output $\\leftarrow c$.\n",
    "    7. Update the count of $s$ in $L_{{\\cal C}[i]}$ and keep sorted it.\n",
    "    8. $H\\leftarrow\\varnothing$.\n",
    "    \n",
    "#### Example ($k=1$)\n",
    "\n",
    "<img src=\"text_coding/TPT_example.svg\" style=\"width: 450px;\"/>\n",
    "\n",
    "### $N$-order decoder\n",
    "\n",
    "1. Steps 1, 2 and 3 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c\\leftarrow$ the next input code.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    3. While $L_{{\\cal C}[i]}[c]=\\varnothing$:\n",
    "        1. $H\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})$.\n",
    "        2. $i\\leftarrow i-1$.\n",
    "    4. $s\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})[c]$.\n",
    "    5. Update the count of $L_{{\\cal C}[i]}[c]$.\n",
    "    6. While $i<k$:\n",
    "        1. $i\\leftarrow i+1$.\n",
    "        2. Insert the symbol $s$ in $L_{{\\cal C}[i]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.8 Unary coding\n",
    "\n",
    "* It is a particular case of the Huffman code where the number of\n",
    "  bits of each code-word (minus one) is equal to the index of the\n",
    "  symbol in the source alphabet. Example:\n",
    "  \n",
    "<img src=\"text_coding/Unary_example.svg\" style=\"width: 100px;\"/>\n",
    "\n",
    "* The unary coding is only optimal when (see Equation\n",
    "  \\ref{eq:symbol_information})\n",
    "  \\begin{equation}\n",
    "    p(s) = 2^{-(s+1)} \\tag{Eq:Unary}\n",
    "    \\label{eq:unary}\n",
    "  \\end{equation}\n",
    "  where $s=0,1,\\cdots$.\n",
    "  \n",
    "<img src=\"text_coding/unary.svg\" style=\"width: 800px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.9 Golomb coding [[Golomb, 1966]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=golomb+1966+run&btnG=)\n",
    "\n",
    "* When the probabilities of the symbols follow an exponential\n",
    "  distribution, the Golomg encoder has the same efficiency than the\n",
    "  Huffman coding, but it is faster. In this case, the probabilities of\n",
    "  the symbols shoud be\n",
    "  \n",
    "  \\begin{equation}\n",
    "    p(s) =\n",
    "    2^{\\displaystyle-\\Big(\\displaystyle m\\big\\lfloor\\displaystyle\\frac{s+m}{m}\\big\\rfloor\\Big)}\n",
    "    \\tag{Eq:Golomb}\n",
    "    \\label{eqw:Golomb}\n",
    "  \\end{equation}\n",
    "  where $s=0,1,\\cdots$ is the symbol and $m=0,1,\\cdots$ is the\n",
    "  ``Golomb slope'' of the distribution.\n",
    "  \n",
    "* For $m=2^k$, it is possible to find a very efficient\n",
    "  implementation and the encoder is also named Rice\n",
    "  encoder~\\cite{Rice79}. In this case\n",
    "  \n",
    "  \\begin{equation}\n",
    "    p(s) =\n",
    "    2^{\\displaystyle-\\Big(2^k \\displaystyle\\big\\lfloor\\displaystyle\\frac{s+2^k}{2^k}\\big\\rfloor\\Big)}\n",
    "    \\tag{Eq:Rice}\n",
    "    \\label{eq:Rice}\n",
    "  \\end{equation}\n",
    "\n",
    "<img src=\"text_coding/Golomb_coding.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "* Notice that for $m=1$, we take the unary encoding.\n",
    "\n",
    "<img src=\"text_coding/Golomb.svg\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. $k\\leftarrow \\lceil\\log_2(m)\\rceil$.\n",
    "2. $r\\leftarrow s~\\mathrm{mod}~m$.\n",
    "3. $t\\leftarrow 2^k-m$.\n",
    "4. Output $(s~\\mathrm{div}~m)$ using an unary code.\n",
    "5. If $r<t$:\n",
    "    1. Output the integer encoded in the $k-1$ least significant bits of $r$ using a binary code.\n",
    "6. Else:\n",
    "    1. $r\\leftarrow r+t$.\n",
    "    2. Output the integer encoded in the $k$ least significant bits of $r$ using a binary code.\n",
    "\n",
    "#### Example ($m=7$ and $s=8$)\n",
    "\n",
    "1. [1] $k\\leftarrow \\lceil\\log_2(8)\\rceil=3$.\n",
    "2. [2] $r\\leftarrow 8 \\text{mod} 7 = 1$.\n",
    "3. [3] $t\\leftarrow 2^3-7 = 8-7 = 1$.\n",
    "4. [4] Output $\\leftarrow 8 \\text{div} 7 = \\lfloor 8/7\\rfloor=1$ as an unary code (2 bits). Therefore, output $\\leftarrow 10$.\n",
    "5. [5] $r=1\\le t=1$.\n",
    "6. [6.A] $r\\leftarrow 1+1=2$.\n",
    "7. [6.B] Output $r=2$ using a binary code of $k=3$ bits. Therefore, $c(8)=10010$.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. $k\\leftarrow\\lceil\\log_2(m)\\rceil$.\n",
    "2. $t\\leftarrow 2^k-m$.\n",
    "3. Let $s\\leftarrow$ the number of consecutive ones in the input (we stop when we read a $0$).\n",
    "4. Let $x\\leftarrow$ the next $k-1$ bits in the input.\n",
    "5. If $x<t$:\n",
    "    1. $s\\leftarrow s\\times m+x$.\n",
    "6. Else:\n",
    "    1. $x\\leftarrow x\\times 2~+$ next input bit.\n",
    "    2. $s\\leftarrow s\\times m+x-t$.\n",
    "    \n",
    "#### Example (decode $10010$ where $m=7$)\n",
    "\n",
    "1. [1] $k\\leftarrow 3$.\n",
    "2. [2] $t\\leftarrow 2^k-m = 2^3-7=1$).\n",
    "3. [3] $s\\leftarrow 1$ because we found only one $1$ in the input.\n",
    "4. [4] $x\\leftarrow \\text{input}_{k-1} = \\text{input}_2 = 01$.\n",
    "5. [5] $x=1\\nless t=1$.\n",
    "6. [6.A] $x\\leftarrow x\\times x\\times 2+\\text{next input bit} = x\\times 1\\times 2+0 = 2$.\n",
    "7. [6.B] $s\\leftarrow s\\times m+x-t = 1\\times 7+2-1=8$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.10 Rice coding\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. $m\\leftarrow 2^k$.\n",
    "2. Output $\\leftarrow\\lfloor s/m\\rfloor$ using an unary code ($\\lfloor s/m\\rfloor+1$ bits).\n",
    "3. Output $\\leftarrow$ the $k$ least significant bits of $s$ using a binary code.\n",
    "    \n",
    "#### Example ($k=1$ and $s=7$)\n",
    "1. [1] $m\\leftarrow 2^k=2^1=2$.\n",
    "2. [2] Output $\\leftarrow \\lfloor s/m\\rfloor=\\lfloor 7/2\\rfloor=3$ using an unary code of 4 bits. Therefore, output $\\leftarrow 1110$.\n",
    "3. Output $\\leftarrow$ the $k=1$ least significant bits of $s=7$\n",
    "  using a unary code ($k+1$ bits). So, output $\\leftarrow 1$. Total\n",
    "  output $c(7)=11101$.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. Let $s$ the number of consecutive ones in the input (we stop when we read a 0).\n",
    "2. Let $x$ the next $k$ input bits.\n",
    "3. $s\\leftarrow s\\times 2^k+x$.\n",
    "\n",
    "#### Example (decode $11101$ where $k=1$)\n",
    "1. [1] $s\\leftarrow 3$ because we found $3$ consecutive ones in the input.\n",
    "2. [2] $x\\leftarrow$ next input $k=1$ input bits. Therefore $x\\leftarrow 1$.\n",
    "3. [3] $s\\leftarrow s\\times 2^k+x = 3\\times 2^1+1 = 6+1 = 7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"text_coding/lena-gray.png\" style=\"width: 400px;\"/>\n",
    "<img src=\"text_coding/peppers-gray.png\" style=\"width: 400px;\"/>\n",
    "<img src=\"text_coding/boats.png\" style=\"width: 400px;\"/>\n",
    "<img src=\"text_coding/zelda.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `rle`, compute the compression ratio of each image as\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{X}{Y}\n",
    "$$\n",
    "\n",
    "where $X$ is the size of the input (the sequence of symbols) and $Y$\n",
    "the size of the output (the code-stream), and populate:\n",
    "```\n",
    "Codec | lena boats pepers zelda Average\n",
    "------+--------------------------------\n",
    "  rle | ....  ....   ....  ....    ....\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
